#!/usr/bin/env python2
"""
This is a utility to recursively search for files by name in a filesystem,
also looking inside archives to an arbitary depth.

While it's usually bad form to create a single monolithic file like this, it's
intended to be used as a single script without some complicated installation
procedure.
"""

import argparse
from collections import deque
from contextlib import closing, contextmanager
from cStringIO import StringIO
from fnmatch import fnmatchcase
import json
import logging
from os import walk
import os.path
import posixpath
import signal
import sys
import tarfile
from tarfile import TarFile
import zipfile
from zipfile import ZipFile
import zlib

import magic

# Interrupt signal (Ctrl+C) handler
def interrupt_handler(signum, stack_frame):
    print "Interrupted"
    sys.exit(1)

# Help text for command-line invocation
PROG_TEXT = (
"Recursively search for files by name, including inside certain kinds of "
"archives. Supported archives include .ZIP, .TAR, .TAR.GZ and .TAR.BZ2"
)

# Leading text for results
RESULT_INDENT = "  > "

# What encoding should we use to interpret command line arguments? Who knows! If
# we get bytes off the command line, it could be sys.stdin.encoding. Or hey,
# maybe it's locale.getpreferredencoding()! Although if we're invoked from
# another program, they could pick anything and we wouldn't know. So let's just
# say it's this.
CLI_ENCODING = 'utf8'

# How many bytes to read to determine the type of file in an archive
MAGIC_BYTES_N = 1024

# Mime types for archives we can search inside
TAR_MIME_TYPES = {
    'application/x-bzip2' : 'r:bz2',
    'application/x-gzip'  : 'r:gz',
    'application/x-tar'   : 'r:'
}

ZIP_MIME_TYPES = ('application/zip', )

def d_cl(some_bytes):
    """ Decode bytes coming in from the command-line. See snarky note for
    CLI_ENCODING.
    """
    return some_bytes.decode(CLI_ENCODING)

class Path(object):
    """ Depending on whether we're searching or presenting results, we'll need
    to access both the split and joined forms of filesystem/archive paths. This
    class does the conversion once and keeps both forms so we don't keep
    converting back and forth.
    """

    @classmethod
    def os(cls, path):
        """ Split a path obtained from OS-related functions. Expects a unicode
        sequence. """
        normed = os.path.normpath(path)
        return cls(os.path.split(normed), normed)

    @classmethod
    def posix(cls, path):
        """ Splits a path that's known to be in POSIX '/'-separated form.
        Expects a unicode sequence. """
        normed = posixpath.normpath(path)
        return cls(posixpath.split(normed), normed)

    def __init__(self, paths, joined):
        """ Stores both the split and joined forms of a path. Expects a unicode
        sequence. """
        self.paths = paths
        self.joined = joined

    def __getitem__(self, key):
        """ Indexing works on the sequence of path components. """
        return self.paths[key]

def concat_paths(paths):
    """ Display paths in a compact form for error messages. """
    return ':'.join(p.joined for p in paths)


# The NamedNode, DirectoryWalker and ArchiveWalker objects all have a "contents"
# member that is iterated over to list ALL files contained within them. The
# exact behaviour of this varies between them.

class NamedNode(object):
    """ Represents a file, whether a normal file or directory or one nested
    inside an archive. The file is denoted by a series of Path objects.

    Files that are directly in the file system have a single entry. Files that
    are nested inside archives have multiple paths (the first is the archive,
    the second is a nested archive, the (N-1)th is the archive containing the
    file, the Nth is the file itself).

    NamedNode objects have an empty "contents" member.
    """

    @classmethod
    def single(cls, path):
        """ Create a NamedNode from a single Path object. """
        return cls((path,))

    def __init__(self, paths):
        """ Create a NamedNode from a sequence of Path objects. """
        self.paths = paths
        self.contents = ()


class DirectoryWalker(object):
    """ Recurses through a filesystem starting at a given directory, looking for
    files in the filesystem itself and any archives it encounters.
    """

    def __init__(self, path):
        """ Creates the directory walker starting at the given directory. The
        list of found files/directories/etc is not populated immediately, but is
        generated dynamically as the "contents" member is iterated over.
        """
        self.path = path
        self.paths = (path,)

    @property
    def contents(self):
        """ The contents of this generator are the files found by walking
        through the filesystem and inspecting any archives found. """
        # Walk through the filesystem, yielding all files and directories found
        for dirpath, dirnames, filenames in walk(self.path.joined):
            # For every file, we return both the file itself as a result, but
            # also inspect it to see if it's an archive.
            for fname in filenames:
                fpath = Path.os(os.path.join(dirpath, fname))

                # The file itself
                yield NamedNode.single(fpath)

                # Inspect it to see if it's an archive
                try:
                    mime_type = magic.from_file(fpath.joined.encode(sys.getfilesystemencoding()), mime=True)
                except IOError as ex:
                    logging.warning(
                        "I/O error on %s (%s)" % (concat_paths((fpath,)), ex.strerror)
                    )
                    mime_type = None

                archive_mode = TAR_MIME_TYPES.get(mime_type, None)
                is_zipfile = mime_type in ZIP_MIME_TYPES

                if archive_mode:
                    # Generate the archive contents too
                    try:
                        yield TarWalker.from_path(fpath, archive_mode)
                    except tarfile.ReadError:
                        # Keep going on exceptions, but warn the user.
                        logging.warning(
                            "Could not open: %s" % concat_paths((fpath,))
                        )
                elif is_zipfile:
                    # Generate the zipfile contents too
                    try:
                        yield ZipWalker.from_path(fpath)
                    except zipfile.BadZipfile:
                        # Keep going on exceptions, but warn the user.
                        logging.warning(
                            "Could not open: %s" % concat_paths((fpath,))
                        )

            # The directories don't need special treatment
            for dname in dirnames:
                yield NamedNode.single(Path.os(os.path.join(dirpath, dname)))


class ArchiveWalker(object):
    """ Recursively searches the given archive, listing the contents and
    inspecting any nested archives for more files. Note that unlike the
    DirectoryWalker, an ArchiveWalker will populate its results as soon as it is
    created (ie. it does not act as a generator).

    This class should not be used directly - there are subclasses for TAR and
    ZIP files that should be used.

    Subclasses should have:

    Class methods: "open_path" and "open_buffer" that return context managers
    for open archives

    Methods:
    "get_members" - list the members of the archive in whatever format is
      reasonable

    "member_path" - construct an encoded path for the given member

    "extract" - extract buffered data for the given member

    "could_be_archive" - return True if the given member is a file that could
      potentially be a nested archive. Normally this just means it's an entry
      for a file and not a directory.
    """

    @classmethod
    def from_path(cls, path, *args, **kargs):
        """ Takes a Path object denoting an archive in the filesystem, and
        recursively lists the files inside (including any inside nested
        archives).
        """
        with cls.open_path(path, *args, **kargs) as archive:
            return cls(archive, (path,))

    @classmethod
    def from_buffer(cls, buf, paths, *args, **kargs):
        """ Takes an already open buffer of archive data, and recursively lists
        the files inside (including any inside nested archives). The caller is
        responsible for closing the buffer. The buffer's position will be
        changed in this function.
        """
        buf.seek(0)
        with cls.open_buffer(buf, *args, **kargs) as archive:
            return cls(archive, paths)

    def __init__(self, archive, paths):
        """ Given an already open archive file, create a recurive list of the
        contents, including files inside archives contained in this one. The
        "paths" argument specifies the location of this archive in the
        filesystem or nested inside other archives. It is the caller's
        responsibility to close the archive.
        """
        self.archive = archive
        self.paths = paths
        self.contents = deque()
        try:
            self._populate()
        except IOError as ex:
            logging.warning(
                "I/O error on %s (%s)" % (concat_paths(self.paths), ex.strerror)
            )

    def _populate(self):
        """ Populates the "contents" member with the recursively found files. It
        is better to do it all at once so the file is not kept open for too
        long.
        """
        # This is a similar approach to that in DirectoryWalker, but the
        # get_members() function returns a flat list rather than the recursive
        # structure of os.walk. We list each file and directory, and if a file
        # is an archive, we get access to the data as a buffer and recurse into
        # it.
        for member in self.get_members():
            # This keeps track of our location in the filesystem and any
            # archives we're already nested inside.
            nested_paths = self.paths + (self.member_path(member),)
            self.contents.append(NamedNode(nested_paths))

            # Inspect files to see if they're archives
            if self.could_be_archive(member):
                with closing(self.extract(member)) as inner_file:
                    magic_bytes = inner_file.read(MAGIC_BYTES_N)
                    inner_mime_type = magic.from_buffer(
                        magic_bytes,
                        mime=True
                    )

                    archive_mode = TAR_MIME_TYPES.get(
                        inner_mime_type,
                        None
                    )

                    is_zipfile = inner_mime_type in ZIP_MIME_TYPES

                    # If it's an archive, create another ArchiveWalker instance
                    # from the extracted buffer
                    if archive_mode:
                        try:
                            self.contents.extend(
                                TarWalker.from_buffer(
                                    inner_file,
                                    nested_paths,
                                    archive_mode
                                ).contents)
                        except (tarfile.ReadError, EOFError) as tferr:
                            # Keep going on exceptions, but warn the user.
                            logging.warning(
                                "Could not open: %s" % concat_paths(nested_paths)
                            )
                    elif is_zipfile:
                        try:
                            self.contents.extend(
                                ZipWalker.from_buffer(
                                    inner_file,
                                    nested_paths
                                ).contents)
                        except (zipfile.BadZipfile, zlib.error) as zferr:
                            # Keep going on exceptions, but warn the user.
                            logging.warning(
                                "Could not open: %s" % concat_paths(nested_paths)
                            )


class TarWalker(ArchiveWalker):
    """ Recursively inspects the contents of TAR archives (including gz or bz2
    compressed archives).
    """

    # By default, the tarfile module will decode entries in a PAX-format archive
    # and then re-encode them based on the current system. It would be better if
    # we could just get the unicode sequence in the first place, but since we
    # can't we'll just force it to re-encode to UTF-8 instead, and then decode
    # it again when we need to.
    ENCODING = 'utf8'

    # The following methods are mostly a wrapper around TarFile

    @classmethod
    def open_path(cls, path, mode='r:*'):
        return TarFile.open(path.joined, mode=mode, encoding=TarWalker.ENCODING)

    @classmethod
    def open_buffer(cls, buf, mode):
        return TarFile.open(fileobj=buf, mode=mode, encoding=TarWalker.ENCODING)

    def get_members(self):
        return self.archive.getmembers()

    def member_path(self, member):
        return Path.posix(member.name.decode(self.ENCODING))

    def extract(self, member):
        return self.archive.extractfile(member)

    def could_be_archive(self, member):
        return member.isfile()


class ZipWalker(ArchiveWalker):
    """ Recursively inspects the contents of ZIP archives. """

    # Mystery! There is no official zipfile encoding, and the archive format
    # doesn't contain it. The Python docs say that WinZip guesses CP-437, so
    # let's use that!
    ENCODING = 'cp437'

    # ZipFile is a little trickier than TarFile, but still fairly simple

    @classmethod
    def open_path(cls, path):
        return ZipFile(path.joined, 'r')

    @classmethod
    def open_buffer(cls, buf):
        return ZipFile(buf, 'r')

    def __init__(self, archive, paths, *args, **kwargs):
        # There are some zip formats that are not supported by the zipfile
        # module (eg. implode). We need to intercept initialisation here so that
        # we can set a flag for later methods to know whether they can extract
        # files or not.
        try:
            archive.testzip()
        except NotImplementedError:
            # This indicates an unsupported archive format
            logging.warning(
                "Extracting contents not supported for %s" % concat_paths(paths)
            )
            self.extractable = False
        else:
            self.extractable = True

        super(ZipWalker, self).__init__(archive, paths, *args, **kwargs)

    def get_members(self):
        return self.archive.infolist()

    def member_path(self, member):
        # Although it's not documented, ZIP file paths are always '/' separated,
        # and directories end in a '/'. So this works for both kinds of archive.
        return Path.posix(member.filename.decode(self.ENCODING))

    def extract(self, member):
        # Unfortunately, ZipFile extraction is not as seamless as it is for
        # TarFile objects. We need to read the file into memory and operate on
        # that buffer, otherwise the extracted file shares a file pointer with
        # the original archive.
        return StringIO(self.archive.read(member))

    def could_be_archive(self, member):
        # Files won't have a trailing '/'
        return self.extractable and not member.filename.endswith('/')


# The following classes deal with filtering the results from a file search.
# They're effectively closures over the text or pattern being searched for.

def MatchAll(_):
    return True

class TextSearcher(object):
    """ Simple, non-wildcard search for text in a file name. Can be case-
    sensitive or case-insensitive.
    """

    def __init__(self, search_text, case_sensitive):
        """ Creates a callable object that will take a sequence of Path objects
        and search for the given text.
        """
        self.search_text = search_text
        self.case_sensitive = case_sensitive

    def __call__(self, paths):
        """ Returns True if the last path in the sequence contains the text. """
        path_tip = paths[-1][-1]
        found = (
            self.case_sensitive and (
                self.search_text.lower() in path_tip.lower()
            )
        ) or (
            self.search_text in path_tip
        )
        return found


class GlobSearcher(object):
    """ UNIX-like glob pattern matching search. Can be case-sensitive or
    case-insenitive.
    """

    def __init__(self, pattern, case_sensitive):
        """ Creates a callable object that will take a sequence of Path objects
        and search for those that match the given pattern.
        """
        self.pattern = pattern
        self.case_sensitive = case_sensitive

    def __call__(self, paths):
        """ Returns True if the last path in the sequence matches the pattern.
        """
        path_tip = paths[-1][-1]
        found = (
            self.case_sensitive and (
                fnmatchcase(path_tip.lower(), self.pattern.lower())
            )
        ) or (
            fnmatchcase(path_tip, self.pattern)
        )
        return found

# The following deal with presenting the results of a search. They're context
# managers because we might need some sort of opening/closing delimiting (eg.
# for JSON or XML).

@contextmanager
def verbose_reporter():
    """ Format and present a matching path, which could be nested to some
    arbitarily depth, in a way that's easy to visually parse. """
    # We don't actually need a context manager here, so just create the function
    # and yield it.
    def reporter(paths):
        print paths[0].joined.encode(CLI_ENCODING)

        if len(paths) > 1:
            indent_level = 0
            for path in paths[1:]:
                leading = " "*indent_level*len(RESULT_INDENT)
                res_string = "%s%s%s" % (leading, RESULT_INDENT, path.joined)
                print res_string.encode(CLI_ENCODING)
                indent_level += 1
    yield reporter


@contextmanager
def json_reporter():
    """ Prepare a JSON encoder for search results. This needs to be a context
    manager so that the list-of-list of paths can be appropriately delimited.
    """
    contents = deque()
    def report_single_result(paths):
        contents.append([p.joined.encode('utf8') for p in paths])
    yield report_single_result
    json.dump(list(contents), sys.stdout, ensure_ascii=False)
    sys.stdout.write('\n')


# Our script needs a starting point, and that could be a directory OR an
# archive. This dict matches up the MIME type to the appropriate constructor.
SEARCH_BASES = {
    'application/x-bzip2' : TarWalker.from_path,
    'application/x-gzip'  : TarWalker.from_path,
    'application/x-tar'   : TarWalker.from_path,
    'application/bzip2'   : TarWalker.from_path,
    'application/gzip'    : TarWalker.from_path,
    'application/tar'     : TarWalker.from_path,
    'application/zip'     : ZipWalker.from_path,
    'inode/directory'     : DirectoryWalker
}

# If the starting point is neither directory nor archive, it's the only result
DEFAULT_SEARCH_BASE = NamedNode.single

def archive_search(base_path, search_func, reporter):
    """ Recursively list all files and directories below the given base path,
    checking inside directories and archives to an arbitrary depth, and
    filtering using the given filter function.

    The base path should be a Path object, and the filter function should take
    a sequence of Path objects and return a boolean if the path is a valid
    result.
    """
    encoded_path = base_path.joined.encode(sys.getfilesystemencoding())
    if os.path.isdir(encoded_path):
        base_path_kind = 'inode/directory'
    else:
        base_path_kind = magic.from_file(encoded_path, mime=True)

    base_path_class = SEARCH_BASES.get(base_path_kind, DEFAULT_SEARCH_BASE)

    try:
        base_walker = base_path_class(base_path)
    except (tarfile.ReadError, zipfile.BadZipfile):
        # We can't do anything here - warn the user and exit
        logging.warning(
            "Could not open: %s" % concat_paths((base_path,))
        )
        return

    to_process = deque((base_walker,))

    # Iterate over a stack of discovered files, filtering as we go
    with reporter() as report_func:
        while to_process:
            node = to_process.pop()
            paths = node.paths

            if search_func(paths):
                report_func(paths)

            # If this node has contents, add them to the stack and keep iterating
            to_process.extendleft(node.contents)


def main(base_path, search_text=None, case_sensitive=False, glob=False, json=False):
    """ Glue between the command-line flags and invoking the search function
    with the appropriate filter function.
    """
    if json:
        reporter = json_reporter
    else:
        reporter = verbose_reporter

    if search_text is None:
        search_func = MatchAll
    elif glob:
        search_func = GlobSearcher(search_text, case_sensitive)
    else:
        search_func = TextSearcher(search_text, case_sensitive)

    archive_search(Path.os(base_path), search_func, reporter)


if __name__ == '__main__':
    # Exit cleanly on Ctrl+C
    signal.signal(signal.SIGINT, interrupt_handler)

    # Get the incoming CLI arguments into shape for the rest of the script by
    # parsing and decoding them.
    parser = argparse.ArgumentParser(
        description=PROG_TEXT
    )
    parser.add_argument('search_path')
    parser.add_argument(
        'search_text', nargs='?',
        help="Text or pattern to search for in file names. Omit to list all "
             "files."
    )
    parser.add_argument('-i', '--ignore-case', action='store_true')
    parser.add_argument('-g', '--glob', action='store_true')
    parser.add_argument('-j', '--json', action='store_true')
    parser.add_argument('-q', '--quiet', action='store_true')

    args = parser.parse_args()

    if args.search_text:
        search_text = d_cl(args.search_text)
    else:
        search_text = None

    if args.quiet:
        log_level = logging.ERROR
    else:
        log_level = logging.WARNING

    # Prepare the logging format for command-line use
    logging.basicConfig(
        format='%(levelname)s %(message)s',
        level=log_level
    )

    main(
        d_cl(args.search_path),
        search_text,
        case_sensitive=args.ignore_case,
        glob=args.glob,
        json=args.json
        )
